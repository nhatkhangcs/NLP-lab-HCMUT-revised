{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Word2Vec with CRNN, CNN, RNN**"]},{"cell_type":"markdown","metadata":{},"source":["Install pyvi for Vietnamese tokenization"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5324,"status":"ok","timestamp":1636622824345,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"RRIs1xe4xp4Q","outputId":"87742e7b-6c78-4496-a2d9-b87586f28658"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyvi in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (0.1.1)\n","Requirement already satisfied: scikit-learn in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (from pyvi) (1.5.2)\n","Requirement already satisfied: sklearn-crfsuite in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (from pyvi) (0.5.0)\n","Requirement already satisfied: numpy>=1.19.5 in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (from scikit-learn->pyvi) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (from scikit-learn->pyvi) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (from scikit-learn->pyvi) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (from scikit-learn->pyvi) (3.5.0)\n","Requirement already satisfied: python-crfsuite>=0.9.7 in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.11)\n","Requirement already satisfied: tabulate>=0.4.2 in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n","Requirement already satisfied: tqdm>=2.0 in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.66.5)\n","Requirement already satisfied: colorama in e:\\general_subjects\\natural language processing\\lab-nlp\\christ\\lib\\site-packages (from tqdm>=2.0->sklearn-crfsuite->pyvi) (0.4.6)\n"]}],"source":["!pip install pyvi"]},{"cell_type":"markdown","metadata":{},"source":["## **Import Libraries**"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"kGnLGj-vwU9t"},"outputs":[],"source":["try:\n","    import tensorflow as tf\n","except ImportError:\n","    !pip install tensorflow\n","    import tensorflow as tf\n","\n","try:\n","    import pandas as pd\n","except ImportError:\n","    !pip install pandas\n","    import pandas as pd\n","\n","try:\n","    import numpy as np\n","except ImportError:\n","    !pip install numpy\n","    import numpy as np\n","\n","try:\n","    from string import digits\n","except ImportError:\n","    !pip install string\n","    from string import digits\n","\n","try:\n","    from collections import Counter\n","except ImportError:\n","    !pip install collections\n","    from collections import Counter\n","\n","try:\n","    from pyvi import ViTokenizer\n","except ImportError:\n","    !pip install pyvi\n","    from pyvi import ViTokenizer\n","\n","try:\n","    from gensim.models.word2vec import Word2Vec\n","except ImportError:\n","    !pip install gensim\n","    from gensim.models.word2vec import Word2Vec\n","\n","try:\n","    from keras.utils import to_categorical\n","except ImportError:\n","    !pip install keras\n","    from keras.utils import to_categorical\n","\n","# This line is specific to Jupyter Notebooks and does not need a try-except block\n","%matplotlib inline"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from utils.helper_function import *"]},{"cell_type":"markdown","metadata":{},"source":["## **Download Data**"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["URLs = {\n","    \"https://drive.google.com/file/d/1q3myiaORcL3fbeks8ExZZcqefFtHthPD/view?usp=drive_link\": \"datasets/vlsp_sentiment_train.csv\",\n","    \"https://drive.google.com/file/d/1jofip_UbAXzzJwrqacVTJ7183mmpBQXe/view?usp=drive_link\": \"datasets/vlsp_sentiment_test.csv\",\n","}\n","\n","for key, value in URLs.items():\n","    download_data(key, value)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Z7z2szHRwU9z","scrolled":true},"outputs":[],"source":["data_train = pd.read_csv(\"datasets/vlsp_sentiment_train.csv\", sep='\\t')\n","data_train.columns =['Class', 'Data']\n","data_test = pd.read_csv(\"datasets/vlsp_sentiment_test.csv\", sep='\\t')\n","data_test.columns =['Class', 'Data']"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1636625507608,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"D7U6TJVNwU90","outputId":"eedc3872-6e77-4919-ed23-8d66ad19d090"},"outputs":[{"name":"stdout","output_type":"stream","text":["(5100, 2)\n","(1050, 2)\n"]}],"source":["print(data_train.shape)\n","print(data_test.shape)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"prV4XxvywU90"},"outputs":[],"source":["labels = data_train.iloc[:, 0].values\n","reviews = data_train.iloc[:, 1].values"]},{"cell_type":"markdown","metadata":{},"source":["One-hot encode the labels"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"hQZymxX0wU91"},"outputs":[],"source":["encoded_labels = []\n","\n","for label in labels:\n","    if label == -1:\n","        encoded_labels.append([1,0,0])\n","    elif label == 0:\n","        encoded_labels.append([0,1,0])\n","    else:\n","        encoded_labels.append([0,0,1])\n","\n","encoded_labels = np.array(encoded_labels)  "]},{"cell_type":"code","execution_count":11,"metadata":{"id":"34J9F70iwU91"},"outputs":[],"source":["reviews_processed = []\n","unlabeled_processed = [] \n","for review in reviews:\n","    review_cool_one = ''.join([char for char in review if char not in digits])\n","    reviews_processed.append(review_cool_one)"]},{"cell_type":"markdown","metadata":{},"source":["Use PyVi for Vietnamese word tokenizer"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"-uevU-I5wU92"},"outputs":[],"source":["word_reviews = []\n","all_words = []\n","for review in reviews_processed:\n","    review = ViTokenizer.tokenize(review.lower())\n","    word_reviews.append(review.split())\n","   "]},{"cell_type":"code","execution_count":13,"metadata":{"id":"VvZBk6kfwU93"},"outputs":[],"source":["EMBEDDING_DIM = 400 # how big is each word vector\n","MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n","MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"]},{"cell_type":"markdown","metadata":{},"source":["Import tokenizer from keras"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"xcAfN337wU93"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"markdown","metadata":{},"source":["Tokenize the Vietnamese text"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"0rXI-fxLwU94"},"outputs":[],"source":["tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n","tokenizer.fit_on_texts(word_reviews)\n","sequences_train = tokenizer.texts_to_sequences(word_reviews)\n","word_index = tokenizer.word_index\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"6wAXC5HrwU94"},"outputs":[],"source":["data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n","labels = encoded_labels"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1636625513548,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"9n7P5xlZwU94","outputId":"593bc5b3-12b5-4013-ab9f-cca7ce907602"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X train and X validation tensor: (5100, 300)\n","Shape of label train and validation tensor: (5100, 3)\n"]}],"source":["print('Shape of X train and X validation tensor:',data.shape)\n","print('Shape of label train and validation tensor:', labels.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Loading the pre-trained Word2Vec model"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8223,"status":"ok","timestamp":1636625591392,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"cFwY64D7wU95","outputId":"92a05467-7a63-43ae-9b08-165e71364523"},"outputs":[],"source":["import gensim\n","from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","\n","word_vectors = KeyedVectors.load_word2vec_format('checkpoints/vi-model-CBOW.bin', binary=True)"]},{"cell_type":"markdown","metadata":{},"source":["Check the vocabulary size"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["vocab size 7919\n"]}],"source":["vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n","embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n","print(\"vocab size\", vocabulary_size)"]},{"cell_type":"markdown","metadata":{},"source":["Create the embedding matrix"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["for word, i in word_index.items():\n","    if i>=MAX_VOCAB_SIZE:\n","        continue\n","    try:\n","        embedding_vector = word_vectors[word]\n","        embedding_matrix[i] = embedding_vector\n","    except KeyError:\n","        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n","\n","del(word_vectors)\n","\n","from keras.layers import Embedding\n","embedding_layer = Embedding(vocabulary_size,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            trainable=True)"]},{"cell_type":"markdown","metadata":{},"source":["Now let's build the CRNN model\n","\n","In this tutorial, we will use the CRNN model to classify the Vietnamese text. The CRNN model is a combination of CNN and RNN. The CNN is used to extract the features from the text, and the RNN is used to classify the text.\n","\n","We will build 3 models:\n","\n","- CNN: Convolutional Neural Network\n","  + The CNN model consists of 3 convolutional layers with max-pooling layers and dropout layers.\n","  + The output of the CNN model is fed into a fully connected layer with a softmax activation function.\n","- RNN: Recurrent Neural Network\n","- CRNN: Convolutional Recurrent Neural Network"]},{"cell_type":"markdown","metadata":{},"source":["Initialize the layers for the CNN model"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["from keras.models import Model\n","from keras.layers import *\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","from keras.models import Model\n","from keras import regularizers\n","sequence_length = data.shape[1]\n","filter_sizes = [3,4,5]\n","num_filters = 100\n","drop = 0.5"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["inputs = Input(shape=(sequence_length,))\n","embedding = embedding_layer(inputs)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(None, 300, 400)\n"]}],"source":["print(embedding.shape)"]},{"cell_type":"markdown","metadata":{},"source":["We can see `print(embedding.shape)` is `(None, 300, 400)`. The first dimension is `None` because the batch size is not defined yet. The second dimension is `300` because the maximum length of the text is 300. The third dimension is `400` because the embedding size is 400."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CNN:\n","    def __init__(self):\n","        self.model "]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1636625591393,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"TF5jaU0-wU95","outputId":"9a57539e-768c-4f5b-ebca-4a51c6d517db"},"outputs":[{"name":"stderr","output_type":"stream","text":["e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n","  warnings.warn(\n"]},{"ename":"ValueError","evalue":"Argument(s) not recognized: {'lr': 0.001}","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[27], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# this creates a model that includes\u001b[39;00m\n\u001b[0;32m     48\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs, output)\n\u001b[1;32m---> 51\u001b[0m adam \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-08\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39madam, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py:62\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     61\u001b[0m ):\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2 \u001b[38;5;241m=\u001b[39m beta_2\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:23\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:90\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[1;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m     )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n","\u001b[1;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 0.001}"]}],"source":["\n","\n","################## LSTM ONLY ###############################\n","# reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n","\n","################# SINGLE LSTM ####################\n","# lstm_0 = LSTM(512)(reshape)\n","\n","# YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n","# lstm_2 = LSTM(1024, return_sequences=True)(reshape)\n","# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n","# lstm_0 = LSTM(256)(lstm_1)\n","\n","############################################################\n","\n","\n","################## CRNN ####################################\n","reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n","\n","conv_0 = Conv1D(num_filters, (filter_sizes[0], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n","conv_1 = Conv1D(num_filters, (filter_sizes[1], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n","conv_2 = Conv1D(num_filters, (filter_sizes[2], ),padding=\"same\",activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n","\n","conv_0 = MaxPool1D(300)(conv_0)\n","conv_1 = MaxPool1D(300)(conv_1)\n","conv_2 = MaxPool1D(300)(conv_2)\n","# Reshape output to match RNN dimension\n","# conv_0 = Reshape((-1, num_filters))(conv_0)\n","# conv_1 = Reshape((-1, num_filters))(conv_1)\n","# conv_2 = Reshape((-1, num_filters))(conv_2)\n","\n","concat = concatenate([conv_0, conv_1, conv_2])\n","concat = Flatten()(concat)\n","# lstm_0 = LSTM(512)(concat)\n","\n","# YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n","# lstm_2 = LSTM(1024, return_sequences=True)(concat)\n","# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n","# lstm_0 = LSTM(256)(lstm_1)\n","\n","############################################################\n","\n","dropout = Dropout(drop)(concat)\n","output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n","\n","# this creates a model that includes\n","model = Model(inputs, output)\n","\n","\n","adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n","model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUZo87gZGYlz"},"outputs":[],"source":["### IF YOU HAVE MODEL WEIGHT AND WANNA LOAD IT\n","model.load_weights(\"model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83275,"status":"ok","timestamp":1636625724079,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"HTJGHh2hwU96","outputId":"d7ad4ba3-e4a5-41b5-8717-2f6b851accfe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","16/16 [==============================] - 7s 358ms/step - loss: 7.3026 - accuracy: 0.4544 - val_loss: 6.5601 - val_accuracy: 0.1912\n","Epoch 2/10\n","16/16 [==============================] - 5s 322ms/step - loss: 5.3777 - accuracy: 0.6275 - val_loss: 6.6515 - val_accuracy: 0.1000\n","Epoch 3/10\n","16/16 [==============================] - 5s 341ms/step - loss: 4.4580 - accuracy: 0.7324 - val_loss: 5.7061 - val_accuracy: 0.1529\n","Epoch 4/10\n","16/16 [==============================] - 5s 320ms/step - loss: 3.8115 - accuracy: 0.7882 - val_loss: 5.7683 - val_accuracy: 0.0598\n","Epoch 5/10\n","16/16 [==============================] - 5s 320ms/step - loss: 3.2973 - accuracy: 0.8436 - val_loss: 5.2096 - val_accuracy: 0.0824\n","Epoch 6/10\n","16/16 [==============================] - 5s 320ms/step - loss: 2.8625 - accuracy: 0.8735 - val_loss: 4.9277 - val_accuracy: 0.0775\n","Epoch 7/10\n","16/16 [==============================] - 5s 321ms/step - loss: 2.5052 - accuracy: 0.9020 - val_loss: 4.4247 - val_accuracy: 0.1049\n","Epoch 8/10\n","16/16 [==============================] - 5s 320ms/step - loss: 2.1981 - accuracy: 0.9142 - val_loss: 3.9065 - val_accuracy: 0.1500\n","Epoch 9/10\n","16/16 [==============================] - 5s 321ms/step - loss: 1.9486 - accuracy: 0.9218 - val_loss: 3.7905 - val_accuracy: 0.1265\n","Epoch 10/10\n","16/16 [==============================] - 5s 320ms/step - loss: 1.7063 - accuracy: 0.9444 - val_loss: 3.6526 - val_accuracy: 0.1069\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f4e6bac3f50>"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["#define callbacks\n","early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n","callbacks_list = [early_stopping]\n","\n","model.fit(data, labels, validation_split=0.2,\n","          epochs=10, batch_size=256, callbacks=callbacks_list, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDWU9oCBwU96"},"outputs":[],"source":["labels_test = data_test.iloc[:, 0].values\n","reviews_test = data_test.iloc[:, 1].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2zBNnGJwU96"},"outputs":[],"source":["encoded_labels_test = []\n","\n","for label_test in labels_test:\n","    if label_test == -1:\n","        encoded_labels_test.append([1,0,0])\n","    elif label_test == 0:\n","        encoded_labels_test.append([0,1,0])\n","    else:\n","        encoded_labels_test.append([0,0,1])\n","\n","encoded_labels_test = np.array(encoded_labels_test)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArJFkP6fwU97"},"outputs":[],"source":["reviews_processed_test = []\n","unlabeled_processed_test = [] \n","for review_test in reviews_test:\n","    review_cool_one = ''.join([char for char in review_test if char not in digits])\n","    reviews_processed_test.append(review_cool_one)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JV9bR1nlwU97"},"outputs":[],"source":["#Use PyVi for Vietnamese word tokenizer\n","word_reviews_test = []\n","all_words = []\n","for review_test in reviews_processed_test:\n","    review_test = ViTokenizer.tokenize(review_test.lower())\n","    word_reviews_test.append(review_test.split())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gvf83C-QwU98"},"outputs":[],"source":["sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n","data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n","labels_test = encoded_labels_test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1636625740257,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"6Zubh3jfwU98","outputId":"de6997b1-8ce4-4430-ad96-bed024518e38"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X train and X validation tensor: (1050, 300)\n","Shape of label train and validation tensor: (1050, 3)\n"]}],"source":["print('Shape of X train and X validation tensor:',data_test.shape)\n","print('Shape of label train and validation tensor:', labels_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":855,"status":"ok","timestamp":1636625742613,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"uzDo7cKqwU98","outputId":"9b01dd3f-f982-41f6-d3b6-61e59120a4b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["33/33 [==============================] - 1s 22ms/step - loss: 2.3534 - accuracy: 0.6133\n"]}],"source":["score = model.evaluate(data_test, labels_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":500,"status":"ok","timestamp":1636625745980,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"cE6zEWGNwU99","outputId":"08bef793-78ce-48b5-80a6-45721ba4d79e"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss: 2.35\n","accuracy: 61.33%\n"]}],"source":["print(\"%s: %.2f\" % (model.metrics_names[0], score[0]))\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1636625423692,"user":{"displayName":"Đức Nguyễn Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGd6hip8b2lawRm1bHdl5fiZElBIJ14NuII7gh=s64","userId":"15255943122151670013"},"user_tz":-420},"id":"-6K00ZjfF-9M","outputId":"a64e89a6-8368-4440-f52c-c337bd98902b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["%cd /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_SlhL45wU99"},"outputs":[],"source":["model.save_weights(\"model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytgukqXFGruD"},"outputs":[],"source":["!cp model.h5 /content/drive/MyDrive"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"word2vec+[crnn, cnn,rnn].ipynb","provenance":[{"file_id":"1srU5584QF_d3NCE9UX9q-6ZgxUTo6FBt","timestamp":1633867144346},{"file_id":"1rFZZf9ECknkLNDv8so_kQNPhqain0RqJ","timestamp":1616587562665}]},"kernelspec":{"display_name":"christ","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
