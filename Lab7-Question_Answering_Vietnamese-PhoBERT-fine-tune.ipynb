{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **QA in Vietnamese with PhoBERT fine-tuning**"]},{"cell_type":"markdown","metadata":{},"source":["First, we need to install required libraries and download the pre-trained PhoBERT model."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"YO6iU2HU82OY"},"outputs":[],"source":["!pip install transformers -q\n","!pip install datasets -q"]},{"cell_type":"markdown","metadata":{"id":"VruGpXaeAY5A"},"source":["### **1. Import libraries**"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8rtcQZMQ9r8Y"},"outputs":[],"source":["import os, sys, argparse, gc\n","from typing import Any\n","\n","try:\n","    import pandas as pd\n","except ImportError:\n","    !pip install pandas\n","    import pandas as pd\n","\n","try:\n","    import numpy as np\n","except ImportError:\n","    !pip install numpy\n","    import numpy as np\n","\n","try:\n","    import tensorflow as tf\n","except ImportError:\n","    !pip install tensorflow\n","    import tensorflow as tf\n","\n","try:\n","    from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, DefaultDataCollator\n","except ImportError:\n","    !pip install transformers\n","    from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, DefaultDataCollator\n","\n","try:\n","    from torch.utils.data import Dataset\n","except ImportError:\n","    !pip install torch\n","    from torch.utils.data import Dataset"]},{"cell_type":"markdown","metadata":{"id":"Jum5gJTe93Hi"},"source":["### **2. Run code**"]},{"cell_type":"markdown","metadata":{},"source":["Check tf version"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QcqppLge953e"},"outputs":[{"name":"stdout","output_type":"stream","text":["TF Version:  2.17.0\n"]}],"source":["print(\"TF Version: \", tf.__version__)\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","    # Currently, memory growth needs to be the same across GPUs\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        # Memory growth must be set before GPUs have been initialized\n","        print(e)"]},{"cell_type":"markdown","metadata":{},"source":["Define model name"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#########################################################\n","MODEL_PHOBERT_BASE = 'vinai/phobert-base'\n","MODEL_PHOBERT_LARGE = 'vinai/phobert-large'\n","#########################################################"]},{"cell_type":"markdown","metadata":{},"source":["Design arguments:\n","- `--model`: model name\n","- `--lr`: learning rate\n","- `--bs`: batch size\n","- `--epochs`: number of epochs\n","- `--maxlen`: maximum length of input\n","- `--stride`: stride\n","- `--use_fast`: use fast tokenizers"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_arguments():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--model\", type=str, default=MODEL_PHOBERT_LARGE, help=\"Pretrained model bert\")\n","    parser.add_argument(\"--lr\", type=float, default=1e-5, help=\"Learning rate\")\n","    parser.add_argument(\"--bs\", type=int, default=16, help=\"Batch size\")\n","    parser.add_argument(\"--epochs\", type=int, default=20, help=\"Number of epochs\")\n","    parser.add_argument(\"--maxlen\", type=int, default=256 , help=\"Max sentence length\")\n","    parser.add_argument(\"--stride\", type=int, default=128, help=\"Stride value for window slide\")\n","    parser.add_argument(\"--use_fast\", type=bool, default=True, help=\"Tokenize sentence with fast bpe\")\n","\n","    return parser.parse_args(args=[])"]},{"cell_type":"markdown","metadata":{},"source":["Data preprocessing"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def read_data(path):\n","    data = pd.read_csv(path)\n","    return data\n","\n","def save_data(data, path):\n","    data.to_csv(path, index=False, encoding='utf-8')\n","\n","data = read_data('datasets/ViWikiQA1.0/ws_train.csv')\n","save_data(data, 'datasets/ViWikiQA1.0/ws_train.csv')\n","\n","data = read_data('datasets/ViWikiQA1.0/ws_dev.csv')\n","save_data(data, 'datasets/ViWikiQA1.0/ws_dev.csv')\n","\n","data = read_data('datasets/ViWikiQA1.0/ws_test.csv')\n","save_data(data, 'datasets/ViWikiQA1.0/ws_test.csv')"]},{"cell_type":"markdown","metadata":{},"source":["Preprocess data:\n","- Our data is in the form of a dictionary with keys: `question`, `context`, `answers_start`, `answers_end`, and `answer`\n","\n","- Need to create columns: `input_ids`, `attention_mask`, `start_positions`, `end_positions`"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#########################################################\n","def preprocess_dataset(ds: pd.DataFrame, tokenizer: Any, maxlen: int):\n","    questions = [q.strip() for q in ds[\"question\"]]\n","    contexts = [str(t) for t in ds[\"context\"]]\n","    inputs = tokenizer(\n","        questions,\n","        contexts,\n","        max_length=maxlen,\n","        truncation=\"only_second\",\n","        return_token_type_ids=True,\n","        padding=\"max_length\",\n","    )\n","\n","    answer_starts = ds[\"answer_start\"]\n","    answer_ends = ds[\"answer_end\"]\n","    answers = ds[\"answer\"]\n","    start_positions = []\n","    end_positions = []\n","    assert len(answer_starts) == len(answer_ends)\n","\n","    for i in range(len(answer_starts)):\n","        start_char = answer_starts[i]\n","        end_char = answer_ends[i]\n","        if start_char == 0 and end_char == 0:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","            continue\n","\n","        answer = answers[i]\n","        context = contexts[i]\n","        input_ids = inputs[\"input_ids\"][i]\n","\n","        # Find the start and end of the context\n","        idx = 0\n","        while input_ids[idx] != 2:\n","            idx += 1\n","        idx += 2\n","        context_start = idx\n","        # print(\"context start:\", context_start, \",id:\", input_ids[context_start])\n","        while idx < len(input_ids) and input_ids[idx] != 1:\n","            idx += 1\n","        context_end = idx - 1\n","        if input_ids[context_end] == 2:\n","            context_end -= 1\n","        # print(\"context end:\", context_end, \"id:\", input_ids[context_end])\n","\n","        pre_ans = tokenizer.encode(context[:start_char], add_special_tokens=False)\n","        ans_ids = tokenizer.encode(answer, add_special_tokens=False)\n","\n","        start_position = context_start + len(pre_ans)\n","        end_position = start_position + len(ans_ids) - 1\n","\n","        # If the answer is not fully inside the context, label is (0, 0)\n","        if start_position < context_start or end_position > context_end:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Otherwise it's the start and end token positions\n","            start_positions.append(start_position)\n","            end_positions.append(end_position)\n","\n","    # print(\"Start positions:\", start_positions)\n","    # print(\"End positions:\", end_positions)\n","\n","    # calculate length of context for each pair of positions\n","    lengths = []\n","\n","    for i in range(len(start_positions)):\n","        if start_positions[i] == 0:\n","            lengths.append(0)\n","        else:\n","            lengths.append(end_positions[i] - start_positions[i] + 1)\n","\n","    # print(\"Lengths:\", lengths)\n","\n","    inputs[\"start_positions\"] = torch.tensor(start_positions, dtype=torch.long)\n","    inputs[\"end_positions\"] = torch.tensor(end_positions, dtype=torch.long)\n","    return inputs"]},{"cell_type":"markdown","metadata":{},"source":["We generate the dataset by using the `datasets` library from Hugging Face. The dataset is then tokenized and preprocessed before being fed into the model."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class QADataset(Dataset):\n","    def __init__(self, inputs):\n","        self.inputs = inputs\n","\n","    def __len__(self):\n","        return len(self.inputs[\"input_ids\"])\n","\n","    def __getitem__(self, idx):\n","        return {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"]},{"cell_type":"markdown","metadata":{},"source":["Print out data information"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def data_info(df):\n","    print(\"Dataframe shape:\", df.shape)\n","    print(\"Columns:\", df.columns)\n","    print(\"Dataframe head:\", df.head())\n","    print(\"Dataframe tail:\", df.tail())\n","    print(\"Dataframe info:\", df.info())\n","    print(\"Dataframe describe:\", df.describe())"]},{"cell_type":"markdown","metadata":{},"source":["Generate the dataset by drop the `title` column and fill the missing values with empty strings"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def generate_dataset(file_name: str, tokenizer: Any, maxlen: int):\n","    df = pd.read_csv(file_name)\n","    df.drop(\"title\", axis=1, inplace=True)\n","    df.fillna({\"answer\": \"\"}, inplace=True)\n","\n","    # data_info(df)\n","    processed_data = preprocess_dataset(df, tokenizer, maxlen)\n","    return QADataset(processed_data)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# #########################################################\n","# class myCallback(tf.keras.callbacks.Callback):\n","#     def __init__(self, saved_model_name: str):\n","#         super().__init__()\n","\n","#         self.min_loss = sys.float_info.max\n","#         self.min_val_loss = sys.float_info.max\n","\n","#         self.saved_model_name = saved_model_name\n","\n","#     def on_epoch_end(self, epoch, logs={}):\n","#         min_loss = logs.get('loss')\n","#         min_val_loss = logs.get('val_loss')\n","\n","#         if min_loss <= self.min_loss and min_val_loss <= self.min_val_loss:\n","#             self.min_loss = min_loss\n","#             self.min_val_loss = min_val_loss\n","\n","#             print(\"\\nsave model at epoch {}\".format(epoch+1))\n","#             # self.model.save(\"models/{}.h5\".format(self.saved_model_name))\n","#             self.model.save(\"model/vinai-phobert-large\", save_format='tf')"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"yU_Zb5qLAjeO"},"outputs":[],"source":["args = get_arguments()\n","model_name = args.model\n","lr = args.lr\n","batch_size = 2 #args.bs\n","epochs = 10 # args.epochs\n","maxlen = args.maxlen\n","stride = args.stride\n","use_fast = args.use_fast"]},{"cell_type":"markdown","metadata":{},"source":["Model information"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["##############################\n","Model : vinai/phobert-large\n","Learning Rate : 1e-05\n","Batch Size : 2\n","Epochs : 10\n","Max Token Length : 256\n","Stride : 128\n","Use Fast : True\n","##############################\n"]}],"source":["print(\"##############################\")\n","print(\"Model :\", model_name)\n","print(\"Learning Rate :\", lr)\n","print(\"Batch Size :\", batch_size)\n","print(\"Epochs :\", epochs)\n","print(\"Max Token Length :\", maxlen)\n","print(\"Stride :\", stride)\n","print(\"Use Fast :\", use_fast)\n","print(\"##############################\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW\n","from torch.utils.data import DataLoader\n","from transformers import get_linear_schedule_with_warmup\n","from torch.cuda.amp import GradScaler, autocast"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Check if CUDA is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["Define collate function"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def collate_fn(batch):\n","    return {\n","        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n","        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n","        'start_positions': torch.tensor([item['start_positions'] for item in batch]),\n","        'end_positions': torch.tensor([item['end_positions'] for item in batch])\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["Define training function. With framework is PyTorch.\n","\n","Can setup the training along with the validation process"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroberta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaForQuestionAnswering\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\utils\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     31\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     36\u001b[0m )\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\utils\\chat_template_utils.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m     42\u001b[0m BASIC_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\torch\\__init__.py:1850\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_library\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;66;03m# quantization depends on torch.fx\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;66;03m# Import quantization\u001b[39;00m\n\u001b[1;32m-> 1850\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantization \u001b[38;5;28;01mas\u001b[39;00m quantization\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;66;03m# Import the quasi random sampler\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quasirandom \u001b[38;5;28;01mas\u001b[39;00m quasirandom\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\torch\\quantization\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobserver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\torch\\quantization\\quantize.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThis file is in the process of migration to `torch/ao/quantization`, and\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mis kept here for compatibility while the migration process is ongoing.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mhere.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     _add_observer_,\n\u001b[0;32m     12\u001b[0m     _convert,\n\u001b[0;32m     13\u001b[0m     _get_observer_dict,\n\u001b[0;32m     14\u001b[0m     _get_unique_devices_,\n\u001b[0;32m     15\u001b[0m     _is_activation_post_process,\n\u001b[0;32m     16\u001b[0m     _observer_forward_hook,\n\u001b[0;32m     17\u001b[0m     _propagate_qconfig_helper,\n\u001b[0;32m     18\u001b[0m     _register_activation_post_process_hook,\n\u001b[0;32m     19\u001b[0m     _remove_activation_post_process,\n\u001b[0;32m     20\u001b[0m     _remove_qconfig,\n\u001b[0;32m     21\u001b[0m     add_quant_dequant,\n\u001b[0;32m     22\u001b[0m     convert,\n\u001b[0;32m     23\u001b[0m     prepare,\n\u001b[0;32m     24\u001b[0m     prepare_qat,\n\u001b[0;32m     25\u001b[0m     propagate_qconfig_,\n\u001b[0;32m     26\u001b[0m     quantize,\n\u001b[0;32m     27\u001b[0m     quantize_dynamic,\n\u001b[0;32m     28\u001b[0m     quantize_qat,\n\u001b[0;32m     29\u001b[0m     swap_module,\n\u001b[0;32m     30\u001b[0m )\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\torch\\ao\\quantization\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# flake8: noqa: F403\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_quantize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuse_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuse_modules  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuse_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuse_modules_qat  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\torch\\ao\\quantization\\fake_quantize.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobserver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     MovingAverageMinMaxObserver,\n\u001b[0;32m      8\u001b[0m     HistogramObserver,\n\u001b[0;32m      9\u001b[0m     MovingAveragePerChannelMinMaxObserver,\n\u001b[0;32m     10\u001b[0m     FixedQParamsObserver,\n\u001b[0;32m     11\u001b[0m     default_fixed_qparams_range_0to1_observer,\n\u001b[0;32m     12\u001b[0m     default_fixed_qparams_range_neg1to1_observer,\n\u001b[0;32m     13\u001b[0m     _with_args,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABC, abstractmethod\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     check_min_max_valid, calculate_qmin_qmax, is_per_tensor, is_per_channel, validate_qmin_qmax)\n\u001b[0;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault_affine_fixed_qparams_observer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault_debug_observer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniformQuantizationObserverBase\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m ]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_PartialWrapper\u001b[39;00m:\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\torch\\ao\\quantization\\utils.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquant_type\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuantType\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Node\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparametrize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_parametrized\n\u001b[0;32m     16\u001b[0m NodePattern \u001b[38;5;241m=\u001b[39m Union[Tuple[Node, Node], Tuple[Node, Tuple[Node, Node]], Any]\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\torch\\fx\\__init__.py:89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproxy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Proxy\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interpreter \u001b[38;5;28;01mas\u001b[39;00m Interpreter, Transformer \u001b[38;5;28;01mas\u001b[39;00m Transformer\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubgraph_rewriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m replace_pattern\n","File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from transformers.models.roberta import RobertaForQuestionAnswering\n","from tqdm import tqdm\n","if __name__ == \"__main__\":\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast)\n","    data_collator = DefaultDataCollator(return_tensors=\"tf\")\n","    dataset_tr = generate_dataset(\n","        \"datasets/ViWikiQA1.0/ws_train.csv\",\n","        tokenizer, maxlen\n","    )\n","    dataset_val = generate_dataset(\n","        \"datasets/ViWikiQA1.0/ws_dev.csv\",\n","        tokenizer, maxlen\n","    )\n","\n","    # Create DataLoader for training and validation datasets\n","    train_loader = DataLoader(dataset_tr, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","    val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","    # Define the initial learning rate\n","    learning_rate = 0.001  # Set an initial learning rate\n","\n","    # Load the pre-trained model for question answering\n","    model: RobertaForQuestionAnswering = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n","\n","    # Prepare the optimizer\n","    optimizer = AdamW(model.parameters(), lr=learning_rate)\n","\n","    # Define a learning rate scheduler\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=len(train_loader) * epochs\n","    )\n","\n","    # Mixed-precision training setup\n","    scaler = torch.amp.GradScaler('cpu')\n","\n","    # Early stopping parameters\n","    early_stop_patience = 3\n","    best_val_loss = float('inf')\n","    early_stop_counter = 0\n","\n","    # Training loop\n","    history = {'loss': [], 'val_loss': []}\n","    # print(train_loader)\n","\n","    print(\"[INFO] Start training...\")\n","    for epoch in tqdm(range(epochs), desc=\"Training\", unit=\"epoch\"):\n","        model.train()\n","        total_loss = 0\n","\n","        # Iterate over training batches\n","        for batch in train_loader:\n","            optimizer.zero_grad()\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            start_positions = batch['start_positions'].to(device)\n","            end_positions = batch['end_positions'].to(device)\n","\n","            # print(f'[DEBUG] input_ids: {input_ids.shape=}')\n","            # print(f'[DEBUG] attention_mask: {attention_mask.shape=}')\n","            # print(f'[DEBUG] start_positions: {start_positions.shape=}')\n","            # print(f'[DEBUG] end_positions: {end_positions.shape=}')\n","\n","            # # check start_positions and end_positions of each \n","            # print(f'[DEBUG] start_positions: {start_positions}')\n","            # print(f'[DEBUG] end_positions: {end_positions}')\n","\n","            # # embedding = model.get_input_embeddings()\n","            # # torch.embedding(embedding., input_ids)\n","\n","            # print(input_ids.max(), input_ids.min())\n","            # print(tokenizer.total_vocab_size)\n","\n","            with torch.amp.autocast(device_type='cpu'):\n","                outputs = model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    start_positions=start_positions,\n","                    end_positions=end_positions\n","                )\n","                loss = outputs.loss\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step()\n","\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        history['loss'].append(avg_loss)\n","\n","        # Validation phase\n","        model.eval()\n","        total_val_loss = 0\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                input_ids = batch['input_ids'].to(device)\n","                attention_mask = batch['attention_mask'].to(device)\n","                start_positions = batch['start_positions'].to(device)\n","                end_positions = batch['end_positions'].to(device)\n","\n","                with torch.amp.autocast(device_type='cpu'):\n","                    outputs = model(\n","                        input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        start_positions=start_positions,\n","                        end_positions=end_positions\n","                    )\n","                    val_loss = outputs.loss\n","\n","                total_val_loss += val_loss.item()\n","\n","\n","        avg_val_loss = total_val_loss / len(val_loader)\n","        history['val_loss'].append(avg_val_loss)\n","\n","        # Early stopping logic\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            early_stop_counter = 0\n","            model.save_pretrained(\"checkpoints/{}\".format(model_name.replace(\"/\", \"-\")))\n","        else:\n","            early_stop_counter += 1\n","\n","        if early_stop_counter >= early_stop_patience:\n","            # print(f\"Early stopping at epoch {epoch + 1}\")\n","            break\n","\n","    # Save training history to CSV\n","    hist_df = pd.DataFrame(history)\n","    hist_df.to_csv(\"checkpoints/plots/{}_bs{}_lr{}.csv\".format(model_name.replace(\"/\", \"-\"), batch_size, learning_rate), index=False)"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"xuMN1FzX0s7Z"},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 18572/18572 [00:32<00:00, 566.62 examples/s]\n","Map: 100%|██████████| 2285/2285 [00:03<00:00, 608.44 examples/s]\n","All model checkpoint layers were used when initializing TFRobertaForQuestionAnswering.\n","\n","Some layers of TFRobertaForQuestionAnswering were not initialized from the model checkpoint at vinai/phobert-large and are newly initialized: ['qa_outputs']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","WARNING:tensorflow:From e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n","\n"]},{"ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nDetected at node tf_roberta_for_question_answering_7/roberta/embeddings/Gather_1 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_4556\\564697334.py\", line 30, in <module>\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1229, in fit\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1804, in fit\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1381, in step_function\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1672, in train_step\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 588, in __call__\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1136, in __call__\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1707, in run_call_with_unpacked_inputs\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 1724, in call\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1136, in __call__\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1707, in run_call_with_unpacked_inputs\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 745, in call\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1136, in __call__\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 164, in call\n\nindices[0,384] = 386 is not in [0, 258)\n\t [[{{node tf_roberta_for_question_answering_7/roberta/embeddings/Gather_1}}]] [Op:__inference_train_function_94272]","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","Cell \u001b[1;32mIn[49], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmixed_precision\u001b[38;5;241m.\u001b[39mset_global_policy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed_float16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m cb \u001b[38;5;241m=\u001b[39m myCallback(model_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m hist \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(history\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[0;32m     37\u001b[0m hist\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/plots/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_bs\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_lr\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m), batch_size, lr))\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32me:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node tf_roberta_for_question_answering_7/roberta/embeddings/Gather_1 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_4556\\564697334.py\", line 30, in <module>\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1229, in fit\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1804, in fit\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1381, in step_function\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1672, in train_step\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 588, in __call__\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1136, in __call__\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1707, in run_call_with_unpacked_inputs\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 1724, in call\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1136, in __call__\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1707, in run_call_with_unpacked_inputs\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 745, in call\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1136, in __call__\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 164, in call\n\nindices[0,384] = 386 is not in [0, 258)\n\t [[{{node tf_roberta_for_question_answering_7/roberta/embeddings/Gather_1}}]] [Op:__inference_train_function_94272]"]}],"source":["# if __name__ == \"__main__\":    \n","#     tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast)\n","#     data_collator = DefaultDataCollator(return_tensors=\"tf\")\n","#     dataset_tr = generate_dataset(\n","#         \"datasets/ViWikiQA1.0/ws_train.csv\",\n","#         tokenizer, data_collator, maxlen,\n","#         stride, batch_size\n","#     )\n","#     dataset_val = generate_dataset(\n","#         \"datasets/ViWikiQA1.0/ws_dev.csv\",\n","#         tokenizer, data_collator, maxlen, stride,\n","#         batch_size\n","#     )\n","\n","#     # Define the learning rate variable\n","#     learning_rate = tf.Variable(0.001, trainable=False)\n","\n","#     # During training, update the learning rate as needed\n","#     # For example, set a new learning rate of 0.0001\n","#     tf.keras.backend.set_value(learning_rate, 0.0001)\n","\n","#     model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)\n","#     model.compile(optimizer= 'adam')\n","\n","#     # Train in mixed-precision float16\n","#     tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n","\n","#     cb = myCallback(model_name.replace(\"/\", \"-\"))\n","\n","#     history = model.fit(\n","#         dataset_tr,\n","#         validation_data=dataset_val,\n","#         epochs=epochs\n","#     )\n","\n","#     hist = pd.DataFrame(history.history)\n","#     hist.to_csv(\"checkpoints/plots/{}_bs{}_lr{}.csv\".format(model_name.replace(\"/\", \"-\"), batch_size, lr))"]},{"cell_type":"markdown","metadata":{"id":"NjtyVuMLO10a"},"source":["## **3. Load Test Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fK_Oj--KPEDL"},"outputs":[],"source":["df_test = pd.read_excel(\"datasets\\Corona_NLP_test.xlsx\")\n","df_test.drop(\"title\", axis=1, inplace=True)\n","df_test.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZeZOX0ezPmVT"},"outputs":[],"source":["df_test[\"context_len\"] = df_test[\"context\"].apply(lambda x: len(x.split()))\n","df_test[\"context_len\"].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxoeE0GDP4gE"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","ds_test = datasets.Dataset.from_dict(df_test)\n","\n","dataset_test = ds_test.map(\n","    lambda ds: preprocess_dataset(ds, tokenizer, maxlen),\n","    batched=True,\n","    remove_columns=ds_test.column_names,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 16\n","test_loader = DataLoader(dataset_test, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"sSpQvZnCRtAy"},"source":["## **4. Load model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKx2yelbRylR"},"outputs":[],"source":["model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n","model.load_state_dict(torch.load(\"checkpoints/{}\".format(model_name.replace(\"/\", \"-\"))))\n","\n","# Set the model to evaluation model\n","model.eval()\n","\n","# Print model architecture summary\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# #########################################################\n","# #MODEL_PHOBERT_BASE = 'vinai/phobert-base'\n","# MODEL_PHOBERT_LARGE = 'vinai/phobert-large'\n","\n","# #########################################################\n","# def get_arguments():\n","#     parser = argparse.ArgumentParser()\n","#     parser.add_argument(\"--model\", type=str, default=MODEL_PHOBERT_LARGE, help=\"Pretrained model bert\")\n","#     parser.add_argument(\"--lr\", type=float, default=1e-5, help=\"Learning rate\")\n","#     parser.add_argument(\"--bs\", type=int, default=16, help=\"Batch size\")\n","#     parser.add_argument(\"--epochs\", type=int, default=20, help=\"Number of epochs\")\n","#     parser.add_argument(\"--maxlen\", type=int, default=512, help=\"Max sentence length\")\n","#     parser.add_argument(\"--stride\", type=int, default=128, help=\"Stride value for window slide\")\n","#     parser.add_argument(\"--use_fast\", type=bool, default=True, help=\"Tokenize sentence with fast bpe\")\n","\n","#     return parser.parse_args(args=[])\n","\n","# #########################################################\n","# def preprocess_dataset(ds: pd.DataFrame, tokenizer: Any, maxlen: int):\n","#     questions = [q.strip() for q in ds[\"question\"]]\n","#     contexts = [str(t) for t in ds[\"context\"]]\n","#     inputs = tokenizer(\n","#         questions,\n","#         contexts,\n","#         max_length=maxlen,\n","#         truncation=\"only_second\",\n","#         return_token_type_ids=True,\n","#         padding=\"max_length\",\n","#     )\n","\n","#     answer_starts = ds[\"answer_start\"]\n","#     answer_ends = ds[\"answer_end\"]\n","#     answers = ds[\"answer\"]\n","#     start_positions = []\n","#     end_positions = []\n","#     assert len(answer_starts) == len(answer_ends)\n","\n","#     for i in range(len(answer_starts)):\n","#         start_char = answer_starts[i]\n","#         end_char = answer_ends[i]\n","#         if start_char == 0 and end_char == 0:\n","#             start_positions.append(0)\n","#             end_positions.append(0)\n","#             continue\n","\n","#         answer = answers[i]\n","#         context = contexts[i]\n","#         input_ids = inputs[\"input_ids\"][i]\n","\n","#         # Find the start and end of the context\n","#         idx = 0\n","#         while input_ids[idx] != 2:\n","#             idx += 1\n","#         idx += 2\n","#         context_start = idx\n","#         # print(\"context start:\", context_start, \",id:\", input_ids[context_start])\n","#         while idx < len(input_ids) and input_ids[idx] != 1:\n","#             idx += 1\n","#         context_end = idx - 1\n","#         if input_ids[context_end] == 2:\n","#             context_end -= 1\n","#         # print(\"context end:\", context_end, \"id:\", input_ids[context_end])\n","\n","#         pre_ans = tokenizer.encode(context[:start_char], add_special_tokens=False)\n","#         ans_ids = tokenizer.encode(answer, add_special_tokens=False)\n","\n","#         start_position = context_start + len(pre_ans)\n","#         end_position = start_position + len(ans_ids) - 1\n","\n","#         # If the answer is not fully inside the context, label is (0, 0)\n","#         if start_position < context_start or end_position > context_end:\n","#             start_positions.append(0)\n","#             end_positions.append(0)\n","#         else:\n","#             # Otherwise it's the start and end token positions\n","#             start_positions.append(start_position)\n","#             end_positions.append(end_position)\n","\n","#     inputs[\"start_positions\"] = np.array(start_positions, dtype=np.int32)\n","#     inputs[\"end_positions\"] = np.array(end_positions, dtype=np.int32)\n","#     return inputs\n","\n","# def generate_dataset(file_name: str, tokenizer: Any, data_collator: Any, maxlen: int, stride: int, batch_size: int, model_name: str):\n","#     df = pd.read_csv(file_name)\n","#     df.drop(\"title\", axis=1, inplace=True)\n","#     df.fillna({\"answer\": \"\"}, inplace=True)\n","#     ds = datasets.Dataset.from_dict(df)\n","\n","#     dataset =  ds.map(\n","#        lambda x: preprocess_dataset(x, tokenizer, maxlen),\n","#        batched=True,\n","#        remove_columns=ds.column_names,\n","#     )\n","    \n","#     return dataset.to_tf_dataset(\n","#         columns=[\n","#             \"input_ids\",\n","#             \"start_positions\",\n","#             \"end_positions\",\n","#             \"attention_mask\",\n","#             \"token_type_ids\",\n","#         ],\n","#         collate_fn=data_collator,\n","#         shuffle=True,\n","#         batch_size=batch_size,\n","#     )\n","\n","#########################################################\n","# class myCallback(tf.keras.callbacks.Callback):\n","#     def __init__(self, saved_model_name: str):\n","#         super().__init__()\n","\n","#         self.min_loss = sys.float_info.max\n","#         self.min_val_loss = sys.float_info.max\n","\n","#         self.saved_model_name = saved_model_name\n","\n","#     def on_epoch_end(self, epoch, logs={}):\n","#         min_loss = logs.get('loss')\n","#         min_val_loss = logs.get('val_loss')\n","\n","#         if min_loss <= self.min_loss and min_val_loss <= self.min_val_loss:\n","#             self.min_loss = min_loss\n","#             self.min_val_loss = min_val_loss\n","\n","#             print(\"\\nsave model at epoch {}\".format(epoch+1))\n","#             # self.model.save(\"models/{}.h5\".format(self.saved_model_name))\n","#             self.model.save(\"/content/gdrive/MyDrive/Research/NLP-Labs/Lab7.1_BERT-Question-and-Answering-Vietnamese/model/vinai-phobert-large\", save_format='tf')\n","            \n"]},{"cell_type":"markdown","metadata":{"id":"9PD9dP_6SXfw"},"source":["## **5. Predict**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrRve7NQSZYR"},"outputs":[],"source":["# Tokenizing the input with PyTorch-compatible tensors\n","def tokenize_question_context(question, context, tokenizer, maxlen, stride):\n","    question = question.strip()\n","    context = context.strip()\n","    inputs = tokenizer(\n","        question,\n","        context,\n","        max_length=maxlen,\n","        truncation=\"only_second\",\n","        stride=stride,\n","        padding=\"max_length\",\n","        return_tensors=\"pt\"\n","    )\n","    return inputs\n","\n","  \n","# Prediction function using a PyTorch model\n","def predict(model, question, context, tokenizer, maxlen, stride):\n","    # Tokenize input\n","    inputs = tokenize_question_context(question, context, tokenizer, maxlen, stride)\n","\n","    # Move input tensors to the model's device (e.g., GPU if available)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    inputs = {key: value.to(device) for key, value in inputs.items()}\n","\n","    # Run the model in evaluation mode\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Extract start and end logits and move to CPU if necessary\n","    start_logits = outputs.start_logits.cpu().numpy()\n","    end_logits = outputs.end_logits.cpu().numpy()\n","\n","    # Get the positions with the highest scores\n","    starts = np.argmax(start_logits, axis=1)\n","    ends = np.argmax(end_logits, axis=1)\n","\n","    # Calculate scores\n","    start_scores = np.max(start_logits, axis=1)\n","    end_scores = np.max(end_logits, axis=1)\n","    scores = start_scores + end_scores\n","\n","    # Filter valid answers\n","    indices = []\n","    for idx, start in enumerate(starts):\n","        end = ends[idx]\n","        if start == 0 and end == 0:  # Skip if no valid answer\n","            continue\n","        if end < start:  # Skip if end is before start\n","            continue\n","        indices.append(idx)\n","\n","    # Decode answers\n","    answers = []\n","    for idx in indices:\n","        score = scores[idx]\n","        ans_ids = inputs[\"input_ids\"][idx][starts[idx]:ends[idx] + 1]\n","        answer = tokenizer.decode(ans_ids, skip_special_tokens=True)\n","        answers.append((answer, score))\n","    \n","    return answers\n","\n","\n","df = df_test[df_test[\"context_len\"] > 500]\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVFcYd08Sndp"},"outputs":[],"source":["df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNCNzpmhStpR"},"outputs":[],"source":["idx = 4  # Select an index from the test DataFrame\n","\n","# Extract question and context for the selected index\n","question = df_test.loc[idx, \"question\"]\n","print(\"Question:\", question)\n","\n","context = df_test.loc[idx, \"context\"]\n","print(\"Context:\", context)\n","\n","# Run the prediction using the adapted PyTorch `predict` function\n","answers = predict(model, question, context, tokenizer, maxlen, stride)\n","\n","# Output the predicted answer(s)\n","print(\"Answer:\", answers)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPEDNxNzF0bkxms+1crucQJ","machine_shape":"hm","mount_file_id":"1uKf8pNtbMRMYCrysu8Dq5843BUSIxHVU","private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"christ","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
