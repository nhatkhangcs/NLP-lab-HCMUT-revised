{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Perform Question Answering on a Pretrained BERT model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPEAbooer2JN"
   },
   "source": [
    "To perform this task, we will use the pretrain model from the **transformer** package. We can install it through the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10844,
     "status": "ok",
     "timestamp": 1674948133402,
     "user": {
      "displayName": "huynh khai",
      "userId": "16623457394309231198"
     },
     "user_tz": -420
    },
    "id": "RRIs1xe4xp4Q",
    "outputId": "65841b2a-854e-492b-e1d6-252826177708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9RT4gZbsHP0"
   },
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kGnLGj-vwU9t"
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tW2ul9e-4kQw"
   },
   "source": [
    "### We use the pretrained BertForQuestionAnswering model 'bert-large-uncased-whole-word-masking-finetuned-squad', more information here: https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vKwxqnqHx3Na"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\General_Subjects\\Natural Language Processing\\Lab-NLP\\christ\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\models--google-bert--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    ")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    \"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKMF737osow7"
   },
   "source": [
    "Define the functions to perform the question answering task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to tokenize the input text and the question. Then we can feed the tokenized input to the model to get the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Implement the function to tokenize the input text and the question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_question_answer(question, answer_text):\n",
    "    \"\"\"\n",
    "    Tokenize the question and answer text into input IDs.\n",
    "\n",
    "    Args:\n",
    "        question (string): The question.\n",
    "        answer_text (string): The paragraph containing the answer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: input_ids, segment_ids\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "    num_seg_a = sep_index + 1\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    segment_ids = [0] * num_seg_a + [1] * num_seg_b\n",
    "\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    return input_ids, segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Implement the function to evaluate the answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to evaluate the start and end positions of the answer in the tokenized input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(input_ids, segment_ids):\n",
    "    \"\"\"\n",
    "    Use the model to predict start and end logits.\n",
    "\n",
    "    Args:\n",
    "        input_ids (list): Tokenized input IDs.\n",
    "        segment_ids (list): Segment IDs distinguishing question and answer text.\n",
    "\n",
    "    Returns:\n",
    "        tuple: start_scores, end_scores\n",
    "    \"\"\"\n",
    "    input_ids_tensor = torch.tensor([input_ids])\n",
    "    segment_ids_tensor = torch.tensor([segment_ids])\n",
    "\n",
    "    outputs = model(input_ids=input_ids_tensor, token_type_ids=segment_ids_tensor)\n",
    "    return outputs.start_logits, outputs.end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model returns the start and end positions of the answer in the tokenized input, we need to convert these positions to the positions in the original input text.\n",
    "\n",
    "Finally, we can return the answer from the original input text using the start and end positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Implement the function to reconstruct the answer from the tokenized input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_answer(start_scores, end_scores, input_ids):\n",
    "    \"\"\"\n",
    "    Reconstruct the answer from start and end scores and the input tokens.\n",
    "\n",
    "    Args:\n",
    "        start_scores (Tensor): The predicted start positions.\n",
    "        end_scores (Tensor): The predicted end positions.\n",
    "        input_ids (list): The tokenized input IDs.\n",
    "\n",
    "    Returns:\n",
    "        string: The predicted answer.\n",
    "    \"\"\"\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        if tokens[i][0:2] == \"##\":\n",
    "            answer += tokens[i][2:]\n",
    "        else:\n",
    "            answer += \" \" + tokens[i]\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the function to perform the question answering task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Implement the function to perform the question answering task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3yKo_OxxyAHe"
   },
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    \"\"\"\n",
    "    Answer the given question based on the answer text.\n",
    "\n",
    "    Args:\n",
    "        question (string): The question.\n",
    "        answer_text (string): The paragraph containing the answer.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    input_ids, segment_ids = tokenize_question_answer(question, answer_text)\n",
    "    start_scores, end_scores = evaluate_model(input_ids, segment_ids)\n",
    "    answer = reconstruct_answer(start_scores, end_scores, input_ids)\n",
    "\n",
    "    print('Question: \"' + question + '\"')\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Test the function with some examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST2kbCQKtJk6"
   },
   "source": [
    "Give it a try with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1674948655401,
     "user": {
      "displayName": "huynh khai",
      "userId": "16623457394309231198"
     },
     "user_tz": -420
    },
    "id": "Z7z2szHRwU9z",
    "outputId": "c7a564f4-a56b-4b08-aa3d-b6e0976bda40",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"what is my dog name?\"\n",
      "Answer: \"ricky\"\n"
     ]
    }
   ],
   "source": [
    "question = \"what is my dog name?\"\n",
    "paragraph = \"I have a dog. It's name is Ricky. I get it at my 15th birthday, when it was a puppy.\"\n",
    "\n",
    "answer_question(question, paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97Zzxn7luMti"
   },
   "source": [
    "Looks good! The model was able to find the correct answer to the question.\n",
    "\n",
    "Let's try with another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1491,
     "status": "ok",
     "timestamp": 1674948713026,
     "user": {
      "displayName": "huynh khai",
      "userId": "16623457394309231198"
     },
     "user_tz": -420
    },
    "id": "D7U6TJVNwU90",
    "outputId": "3285d2e4-8d44-4ba6-dae2-7e2575da98fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"when Leonhard Euler was born?\"\n",
      "Answer: \"15 april 1707\"\n"
     ]
    }
   ],
   "source": [
    "question = \"when Leonhard Euler was born?\"\n",
    "paragraph = \"Leonhard Euler: 15 April 1707 – 18 September 1783 was a Swiss mathematician, \\\n",
    "physicist, astronomer, geographer, logician and engineer who made important and influential discoveries in many branches of mathematics, \\\n",
    "such as infinitesimal calculus and graph theory, \\\n",
    "while also making pioneering contributions to several branches such as topology and analytic number theory. \\\n",
    "He also introduced much of the modern mathematical terminology and notation, \\\n",
    "particularly for mathematical analysis, such as the notion of a mathematical function.[4] He is also known for his work in mechanics, fluid dynamics, optics, astronomy and music theory\"\n",
    "\n",
    "answer_question(question, paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with a more complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Picasso was born at 23:15 on 25 October 1881, in the city of Málaga, Andalusia, in southern Spain. \\\n",
    "He was the first child of Don José Ruiz y Blasco (1838–1913) and María Picasso y López. Picasso's family was of middle-class background. \\\n",
    "His father was a painter who specialized in naturalistic depictions of birds and other game. For most of his life, Ruiz was a professor of art at the School of Crafts and a curator of a local museum. \\\n",
    "Ruiz's ancestors were minor aristocrats.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: What is Picasso's father job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1435,
     "status": "ok",
     "timestamp": 1674949147261,
     "user": {
      "displayName": "huynh khai",
      "userId": "16623457394309231198"
     },
     "user_tz": -420
    },
    "id": "vs1P4t5TvjDY",
    "outputId": "1c4b5216-612c-4cfa-fa08-594f56f9ca51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"what is Picasso's father job\"\n",
      "Answer: \"a painter\"\n"
     ]
    }
   ],
   "source": [
    "question = \"what is Picasso's father job\"\n",
    "answer_question(question, paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: What is the occupation of Picasso's father?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2014,
     "status": "ok",
     "timestamp": 1674949240926,
     "user": {
      "displayName": "huynh khai",
      "userId": "16623457394309231198"
     },
     "user_tz": -420
    },
    "id": "LsNf8OJ-wRPY",
    "outputId": "7b5a9736-fc15-4b20-e7a5-c7ac66144eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"what is the occupation of Picasso's father\"\n",
      "Answer: \"painter\"\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the occupation of Picasso's father\"\n",
    "answer_question(question, paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: What is his mother's name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2961,
     "status": "ok",
     "timestamp": 1674949287154,
     "user": {
      "displayName": "huynh khai",
      "userId": "16623457394309231198"
     },
     "user_tz": -420
    },
    "id": "yK694qJowVPo",
    "outputId": "8a5205b1-904b-40ea-bc46-54e3ccf8b92a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"what is his mother's name\"\n",
      "Answer: \"maria picasso y lopez\"\n"
     ]
    }
   ],
   "source": [
    "question = \"what is his mother's name\"\n",
    "answer_question(question, paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: What is Picasso's family like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1671,
     "status": "ok",
     "timestamp": 1674949354501,
     "user": {
      "displayName": "huynh khai",
      "userId": "16623457394309231198"
     },
     "user_tz": -420
    },
    "id": "HqnkwnhvwjTg",
    "outputId": "d7355233-e68a-4b58-9317-a3b34448a48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"what is Picasso's family like\"\n",
      "Answer: \"middle - class background\"\n"
     ]
    }
   ],
   "source": [
    "question = \"what is Picasso's family like\"\n",
    "answer_question(question, paragraph)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1srU5584QF_d3NCE9UX9q-6ZgxUTo6FBt",
     "timestamp": 1633867144346
    },
    {
     "file_id": "1rFZZf9ECknkLNDv8so_kQNPhqain0RqJ",
     "timestamp": 1616587562665
    }
   ]
  },
  "kernelspec": {
   "display_name": "christ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
